{"path":"docs/学校课程/归档课程/大数据/课件/06 MapReduce Basic Programming (I).pdf","text":"MapReduce基础算法程序设计 (I) 摘要 p MapReduce可解决哪些算法问题？ p 回顾：MapReduce流水线 p MapReduce WordCount1.0 p MapReduce WordCount2.0 p MapReduce矩阵乘法 p MapReduce关系代数运算 2 应用范围 ¨ 自MapReduce发明后，Google大量用于各种海量数据处理。目前Google内部有7千以上的程序 基于MapReduce实现。MapReduce可广泛应用于搜索引擎（文档倒排索引，网页链接图分析与 页面排序等）、Web日志分析、文档分析处理、机器学习、机器翻译等各种大规模数据并行 计算应用领域各类大规模数据并行处理算法。 3 基本算法 各种全局数据相关性小、能适当划分数据的计算任务，如： ¨ 分布式排序 ¨ 分布式GREP(文本匹配查找) ¨ 关系代数操作 如：选择，投影，求交集、并集，连接，成组，聚合… ¨ 矩阵向量相乘、矩阵相乘 ¨ 词频统计(word count)，词频重要性分析(TF-IDF) ¨ 单词同现关系分析 典型的应用如从生物医学文献中自动挖掘基因交互作用关系 ¨ 文档倒排索引 ¨ …… 4 复杂算法及应用  Web搜索引擎 ¤ 网页爬取、倒排索引、网页排序、搜索算法  Web访问日志分析 ¤ 分析和挖掘用户在Web上的访问、购物行为特征、以定制个性化用户界面或投放用户感兴趣的产品广 告  数据/文本统计分析 ¤ 如科技文献引用关系分析和统计、专利文献引用分析和统计  图算法 ¤ 并行化宽度优先搜索(最短路径问题，可克服Dijkstra串行算法的不足)，最小生成树，子树搜索、比对 ¤ Web链接图分析算法PageRank，垃圾邮件连接分析  聚类(clustering) ¤ 文档聚类、图聚类、其它数据集聚类 5 复杂算法及应用  相似性比较分析算法 ¤ 字符序列、文档、图、数据集相似性比较分析  基于统计的文本处理 ¤ 最大期望(EM)统计模型，隐马可夫模型(HMM)，……  机器学习 ¤ 监督学习、无监督学习、分类算法(决策树、SVM…)  数据挖掘  统计机器翻译  生物信息处理 ¤ DNA序列分析比对算法Blast：双序列比对、多序列比对 ¤ 生物网络功能模块(Motif)查找和比对  广告推送与推荐系统  …… 6 MapReduce算法应用专著 1.Mining of Massive Datasets 2010, Jure Leskovec (Stanford Univ.), Anand Rajaraman (Kosmix, Inc), Jeffrey D. Ullman (Stanford Univ.) 主要介绍基于MapReduce的大规模数据挖掘相关的技术和算法，尤其是Web或者从Web导出的 数据 7 Ch3. Similarity search, including the key techniques of minhashing and locality- sensitive hashing. Ch4. Data-stream processing and specialized algorithms for dealing with data that arrives so fast it must be processed immediately or lost. Ch5. The technology of search engines, including Google’s PageRank, link-spam detection, and the hubs-and-authorities approach(a link analysis algorithm： Hyperlink-Induced Topic Search (HITS)). Ch6. Frequent-itemset mining, including association rules, market-baskets, the A-Priori Algorithm and its improvements (a classic algorithm for learning association rules). Ch7. Algorithms for clustering very large, high-dimensional datasets. Ch8. Two key problems for Web applications: managing advertising and recommendation systems. MapReduce算法应用专著 2. Data-Intensive Text Processing with MapReduce Jimmy Lin and Chris Dyer，2010，University of Maryland, College Park 主要介绍基于MapReduce的大规模文档数据处理技术和算法 8 Ch4. Inverted Indexing for Text Retrieval Ch5. Graph Algorithms Parallel Breadth-First Search PageRank Ch6. EM Algorithms for Text Processing EM, HMM Case Study: Word Alignment for Statistical Machine Translation 回顾：MapReduce流水线 MapReduce Pipeline map(K1,V1)->[(K2,V2)] shuffle and sort reduce(K2,[V2]) -> [(K3,V3)] ([…] denotes a list ) Any algorithm that you wish to develop must be expressed in terms of such rigidly-defined components 9 回顾：MapReduce流水线 ¨ Mapper ¤ Initialize: setup() ¤ map(): It is called once for each key/value pair in the input split. The default is the identity function. ¤ Close: cleanup() ¨ Shuffle ¤ Shuffle phase needs the Partitioner to route the output of mapper to reducer. ¤ Partitioner controls the partitioning of the keys of the intermediate map-outputs. The key is used to derive the partition, typically by a hash function. The total number of partitions is the same as the number of reduce tasks for the job. ¤ HashPartitioner is the default Partitioner. 10 回顾：MapReduce流水线 ¨ Sort ¤ We can controls how the keys are sorted before they are passed to the Reducer by using a customized comparator. ¨ Reducer ¤ Initialize: setup() ¤ reduce(): It is called once for each key. The default implementation is an identity function. ¤ Close: cleanup() 11 常用数据类型 ¨ 这些数据类型都实现了WritableComparable接口，以便进行网络传输和文件存 储，以及进行大小比较。 12 Hadoop API文档 ¨ Apache Hadoop Main 3.3.6 API ¤ Common; HDFS; MapReduce; YARN ¤ https://hadoop.apache.org/docs/stable/api/index.html 13 MapReduce User Interface ¨ Mapper ¤ map n How many maps？ ¤ combine ¨ Reducer ¤ shuffle ¤ sort/secondary sort ¤ Reduce n How many reduces？ ¨ Partitioner ¨ Counter 14 MapReduce User Interface ¨ Job Configuration ¤ Job represents a MapReduce job configuration ¨ Task Execution & Environment ¤ The MRAppMaster executes the Mappper/Reducer task as a child process in a separate JVM. ¨ Job Submission and Monitoring ¤ Job.submit() or Job.waitForCompletion(boolean) ¨ Job Input ¨ Job Output 15 MapReduce User Interface ¨ Other Useful Features ¤ Submitting Jobs to Queues ¤ Counters: global counters ¤ DistributedCache: distribute application-specific, large, read-only files efficiently ¤ Profiling ¤ Debugging ¤ Data Compression ¤ Skipping Bad Records ¤ …… 16 MapReduce基本工作过程 17 主要组件 ¨ 文件输入格式InputFormat  定义了数据文件如何分割和读取  InputFormat提供了以下一些功能  选择文件或者其它对象，用来作为输入  定义InputSplits，将一个文件分开成为任务  为RecordReader提供一个工厂，用来读取这个文件  有一个抽象的类FileInputFormat，所有的输入格式类都从这个类继承这个类的功能以及特性。 当启动一个Hadoop任务的时候，一个输入文件所在的目录被输入到FileInputFormat对象中。 FileInputFormat从这个目录中读取所有文件。然后FileInputFormat将这些文件分割为一个或者多 个InputSplits。  通过在JobConf对象上设置JobConf.setInputFormat设置文件输入的格式 18 主要组件 ¨ 文件输入格式InputFormat 19 InputFormat: Description: Key: Value: TextInputFormat Default format; reads lines of text files The byte offset of the line The line contents KeyValueTextInput Format Parses lines into key-val pairs Everything up to the first tab character The remainder of the line SequenceFileInputFormat A Hadoop-specific high- performance binary format user-defined user-defined 主要组件 ¨ 输入数据分块InputSplits  InputSplit定义了输入到单个Map任务的输入数据  一个MapReduce程序被统称为一个Job，可能有 上百个任务构成  InputSplit将文件分为64MB的大小  配置文件hadoop-site.xml中的 mapred.min.split.size参数控制这个大小  mapred.tasktracker.map.task.maximum用来控制某 一个节点上所有map任务的最大数目 20 主要组件 21 关于Split（分片） HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split是一个 逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法 完全由用户自己决定。 主要组件 22 Reduce任务的数量 Ø 最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 Ø 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源 处理可能发生的错误） Map任务的数量 Ø Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下， 理想的分片大小是一个HDFS块 主要组件 ¨ 数据记录读入RecordReader  InputSplit定义了一项工作的大小，但是没有定 义如何读取数据  RecordReader实际上定义了如何从数据上转化 为一个(key,value)对的详细方法，并将数据输 出到Mapper类中  TextInputFormat提供了LineRecordReader 23 主要组件 ¨ Mapper  每一个Mapper类的实例生成了一个Java进 程（在某一个InputSplit上执行）  有两个额外的参数OutputCollector以及 Reporter，前者用来收集中间结果，后者用 来获得环境参数以及设置当前执行的状态。 24 主要组件 ¨ Combiner ¤ 合并相同key的键值对，减少partitioner时候的数据通信开销； ¤ 是在本地执行的一个Reducer，满足一定的条件才能够执行。  conf.setCombinerClass(Reduce.class); 25 主要组件 ¨ Partitioner & Shuffle ¤ 在Map工作完成之后，每一个 Map函数会将结果传到对应的Reducer所在 的节点，此时，用户可以提供一个Partitioner类，用来决定一个给定的 (key,value)对传输的具体位置。 26 主要组件 ¨ Sort ¤ 传输到每一个节点上的所有的Reduce函数接收到的(key,value)都会被Hadoop 自动排序（即Map生成的结果传送到某一个节点的时候，会被自动排序） 27 主要组件 28 输入 缓存 溢写（分区、排序、合并） 数据被Reduce 任务取走 磁盘文件归并 多个分区 归并 归并 输出 Reduce任务Map任务 其他Map任务 其他Reduce任务 Map Reduce 1. Shuffle过程简介 主要组件 29 2. Map端的Shuffle过程 Map任务 缓存 1 2 输入数据和执行Map任务 写入缓存 3 溢写（分区、排序、合并） 4 文件归并 Ø 每个Map任务分配一个缓存 Ø MapReduce默认100MB缓存 Ø 设置溢写比例0.8 Ø 分区默认采用哈希函数 Ø 排序是默认的操作 Ø 排序后可以合并（Combine） Ø 合并不能改变最终结果 Ø 在Map任务全部结束之前进行归并 Ø 归并得到一个大的文件，放在本地磁盘 Ø 文件归并时，如果溢写文件数量大于预定值（默认是 3）则可以再次启动Combiner，少于3不需要 Ø JobTracker会一直监测Map任务的执行，并通知 Reduce任务来领取数据 合并（Combine）和归并（Merge）的区别： 两个键值对<“a”,1>和<“a”,1>，如果合并，会得到<“a”,2>，如果归并，会得到<“a”,<1,1>> 主要组件 30 3. Reduce端的Shuffle过程 缓存 Map任务 Reduce任务 磁盘 磁盘 文件归并 其他Map任务 “ 领取” 数据 归并数据 把数据输入给 Reduce任务 1 2 3 其他Reduce任务 分区 分区 其他Reduce任务 Ø Reduce任务向JobTracker询问Map任务是否已经完成，若完成，则领取数据 Ø Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘 Ø 多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的 Ø 当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce 主要组件 ¨ Reducer  执行用户定义的Reduce操作  接收到一个OutputCollector的类作为输出 31 主要组件 ¨ 文件输出格式OutputFormat  写入到HDFS的所有OutputFormat都继承自 FileOutputFormat  每一个Reducer都写一个文件到一个共同的输 出目录，文件名是part-nnnnn，其中nnnnn是 与每一个reducer相关的一个号（partition id）  FileOutputFormat.setOutputPath()  JobConf.setOutputFormat() 32 主要组件 ¨ 文件输出格式OutputFormat ¨ RecordWriter ¤ TextOutputFormat实现了缺省的LineRecordWriter，以”key\\t value”形式输出一 行结果 33 OutputFormat: Description TextOutputFormat Default; writes lines in \"key \\t value\" form SequenceFileOutputFormat Writes binary files suitable for reading into subsequent MapReduce jobs NullOutputFormat Disregards its inputs MapReduce WordCount1.0 ¨ 基本数据处理流程 34 MapReduce WordCount1.0 ¨ 程序员主要的编码工作如下： ¤ 实现Map类 ¤ 实现Reduce类 ¤ 实现main函数（运行Job） 35 MapReduce WordCount1.0 ¤ 实现Map类 ¤ 这个类实现 org.apache.hadoop.mapreduce.Mapper中的 map 方法，输入参数中的 value 是文本文件中的一行，利用 StringTokenizer 将这个字符串拆成单词，然后通过context.w rite收集<key, value>对。 ¤ 代码中 LongWritable, IntWritable, Text 均是 Hadoop 中实现的用于封装 Java 数据类型的 类，这些类都能够被串行化从而便于在分布式环境中进行数据交换，可以将它们分别 视为 long, int, String 的替代。 MapReduce WordCount1.0 ¨ Map类代码 public static class TokenizerMapper //定义Map类实现字符串分解 extends Mapper<Object, Text, Text, IntWritable>{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); //实现map()函数 public void map(Object key, Text value, Context context) throws IOException, InterruptedException { //将字符串拆解成单词 StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); //将分解后的一个单词写入word context.write(word, one); //收集<key, value> } } } MapReduce WordCount1.0 ¨ 实现Reduce类 ¤ 这个类实现org.apache.hadoop.mapreduce.Reducer中的 reduce 方法，输入参数中的(key, values) 是由 Map 任务输出的中间结果，values 是一个Iterator，遍历这个 Iterator，就可 以得到属于同一个 key的所有value。 ¤ 此处key 是一个单词，value 是词频。只需要将所有的 value 相加，就可以得到这个单 词的总的出现次数。 MapReduce WordCount1.0 ¨ Reduce类代码 //Reducekeyvalue public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> { private IntWritable result = new IntWritable(); //reduce() public void reduce(Text key, Iterable<IntWritable> values, Context context ) throws IOException, InterruptedException { int sum = 0; //valueskeyvalue for (IntWritable val : values) { sum += val.get(); } result.set(sum); //<key, value> context.write(key, result); } } MapReduce WordCount1.0 ¨ 实现main函数（运行Job） ¤ 在 Hadoop 中一次计算任务称之为一个 Job，main函数主要负责新建一个Job对象并为 之设定相应的Mapper和Reducer类，以及输入、输出路径等。 MapReduce WordCount1.0 ¨ main函数代码 public static void main(String[] args) throws Exception{ //为任务设定配置文件 Configuration conf = new Configuration(); //命令行参数 String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length != 2){ System.err.println(\"Usage: wordcount <in> <out>\"); System.exit(2); } Job job = new Job(conf, “word count”); //新建一个用户定义的Job job.setJarByClass(WordCount.class); //设置执行任务的jar job.setMapperClass(TokenizerMapper.class); //设置Mapper类 job.setCombinerClass(IntSumReducer.class); //设置Combine类 job.setReducerClass(IntSumReducer.class); //设置Reducer类 job.setOutputKeyClass(Text.class); //设置job输出的key //设置job输出的value job.setOutputValueClass(IntWritable.class); //设置输入文件的路径 FileInputFormat.addInputPath(job, new Path(otherArgs[0])); //设置输出文件的路径 FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); //提交任务并等待任务完成 System.exit(job.waitForCompletion(true) ? 0 : 1); } MapReduce WordCount1.0 ¨ 编译源代码 ¤ 完成编译 ¤ 导出jar文件 n 导出wordcount程序的jar包 n 导出jar文件的时候可以指定一个 主类MainClass，作为默认执行的 一个类 42 IntelliJ IDEA CE MapReduce WordCount1.0 ¨ 本地运行调试 ¤ 将程序复制到本地Hadoop系统的执行目录，并准备一个小的测试 数据，即可通过hadoop的安装包进行运行调试 bin/hadoop fs -mkdir input bin/hadoop fs -put docs/*.html input bin/hadoop jar example.jar wordcount input output ¤ 当需要用集群进行海量数据处理时，在本地程序调试正确 运行后，可按照前述的远程作业提交步骤，将作业提交到 远程hadoop集群上运行。 43 开发环境与工具：VS Code 44 安装Java开发插件和Maven插件 开发环境与工具：VS Code 45 编写、编译、打包、运行Java程序 MapReduce WordCount 2.0 ¨ 1. 忽略大小写 ¨ 2. 忽略标点符号 46 MapReduce WordCount 2.0 ¨ Mapper (new) 47 MapReduce WordCount 2.0 ¨ Mapper (new) 48 MapReduce WordCount 2.0 ¨ Mapper (updated) 49 MapReduce WordCount 2.0 ¨ Reducer 50 MapReduce WordCount 2.0 ¨ main函数 51 MapReduce WordCount 2.0 ¨ Input ¤ File01: Hello World Bye World ¤ File02: Hello Hadoop Goodbye Hadoop ¤ File03: Hello World, Bye World! ¤ File04: Hello Hadoop, Goodbye to hadoop. ¨ 运行 $bin/hadoop jar wc2.jar input output2 ¨ 结果： 52 MapReduce WordCount 2.0 ¨ 准备patterns文件 ¨ 再次运行 ¤ $ bin/hadoop jar wc.jar - Dwordcount.case.sensitive=true input output3 -skip wordcount/patterns.txt ¨ 输出： 53 ¨ 再次运行 ¤ $ bin/hadoop jar wc.jar - Dwordcount.case.sensitive=false input output4 -skip wordcount/patterns.txt ¨ 输出： MapReduce WordCount 2.0 ¨ Demonstrates how applications can access configuration parameters in the setup method of the Mapper (and Reducer) implementations. ¨ Demonstrates how the DistributedCache can be used to distribute read-only data needed by the jobs. Here it allows the user to specify word-patterns to skip while counting. ¨ Demonstrates the utility of the GenericOptionsParser to handle generic Hadoop command-line options. ¨ Demonstrates how applications can use Counters and how they can set application- specific status information passed to the map (and reduce) method. 54 MapReduce WordCount in Python ¨ Hadoop Streaming ¤ Hadoop Streaming是Hadoop的一个工具， 它帮助用户创建和运行一类 特殊的map/reduce作业， 这些特殊的map/reduce作业是由一些可执行 文件或脚本文件充当mapper或者reducer。 ¤ Hadoop Streaming 参考文档 n https://hadoop.apache.org/docs/current/hadoop- streaming/HadoopStreaming.html 55 Hadoop Streaming ¨ 使用方式 ¤ $HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/.../hadoop-streaming.jar [genericOptions] [streamingOptions] ¤ $HADOOP_HOME/bin/ mapred streaming [genericOptions] [streamingOptions] ¨ 常用参数 ¤ -file ¤ -mapper ¤ -reducer ¤ -input ¤ -output 56 Hadoop Streaming ¨ 使用 Hadoop Streaming 来让数据在map 和 reduce之间传递数据，使用sys.stdin 获得输入数据，sys.stdout 作为标准输出。 ¨ mapper和reducer都是可执行文件，它们从标准输入读入数据（一行一行读）， 并把计算结果发给标准输出。Streaming工具会创建一个Map/Reduce作业， 并 把它发送给合适的集群，同时监视这个作业的整个执行过程。 ¨ 用户可以设定stream.non.zero.exit.is.failure为true 或false 来表明streaming task的 返回值非零时是 Failure 还是Success。默认情况，streaming task返回非零时表示 失败。 57 Hadoop Streaming 58 mapper.py 59 reducer.py 60 运行 #!/bin/bash bin/hdfs dfs -rm -r python/output bin/hadoop jar ./share/hadoop/tools/lib/hadoop-streaming-2.7.4.jar \\ -D stream.non.zero.exit.is.failure=false \\ -D mapred.job.name=python_word_count \\ -file ./mapper.py -mapper \"python mapper.py\" \\ -file ./reducer.py -reducer \"python reducer.py\" \\ -input python/input/wordcount_data.txt -output python/output 61 MapReduce矩阵乘法 ¨ 并行化矩阵乘法 ¨ 矩阵Mab和矩阵Nbc的乘积P=M·N ¤ 𝑃!\" = (𝑀 % 𝑁)!\"= ∑𝑚!#𝑛#\" ¨ Map阶段：进行数据准备 ¤ 定义(key, value)对 ¤ 矩阵M：<(i, k), (M, j, mij)> ¤ 矩阵N：<(i, k), (N, j, njk)> ¤ 其中i=1, 2, …a; j=1,2,…b; k=1, 2, …c 62 MapReduce矩阵乘法 ¨ Reduce阶段：mij*njk ¤ 对于每个键(i, k)相关联的值(M, j, mij)及(N, j, njk)，根据相同j值将mij和 njk分别存入不同数组中，然后将两者的第j个元素抽取出来分别 相乘，最后相加，即可得到pik的值。 ¨ 代码示例 63 MapReduce矩阵乘法 ¨ 举例： ¨ M 1,2 N 2 1 3 0 2 4 ¨ i=1 j=1,2 k=1,2,3 ¨ Map输出 n <(1,1),(M,1,1)><(1,1),(N,1,2)><(1,1),(M,2,2)><(1,1),(N,2,0)> n <(1,2),(M,1,1)><(1,2),(N,1,1)><(1,2),(M,2,2)><(1,2),(N,2,2)> n <(1,3),(M,1,1)><(1,3),(N,1,3)><(1,3),(M,2,2)><(1,3),(N,2,4)> ¨ Reduce输出 n <(1,1),2> <(1,2),5><(1,3),11> 64 关系代数运算 ¨ 选择、投影、并、交、差以及自然连接操作 65 ID NAME AGE GRADE 1 张小雅 20 91 2 刘伟 19 87 3 李婷 21 82 4 孙强 20 95 表1 关系R ID GENDER HEIGHT 1 女 165 2 男 178 3 女 170 4 男 175 表2 关系S 关系代数运算 ¨ 选择操作 ¤ Map阶段：对于每个输入的记录判断是否满足条件，将满足条件的记 录输出为(Rc, null)。 ¤ Reduce阶段：无需做额外工作 ¤ setNumReduceTasks(0) ¨ 投影操作 ¤ Map 阶段：将每条记录在该属性上的值作为键输出即可。 ¤ Reduce阶段：将Map端输入的键输出即可。 66 关系代数运算 ¨ 交运算 ¤ 同一个模式的关系R和关系T求交集 ¤ Map阶段：每条记录r输出为(r, 1) ¤ Reduce阶段：如果计数为2则输出该记录 ¤ 注意事项：R和T的相同记录要发送到同一个Reduce节点 n 重写hashCode()方法使得具有相同域值的记录具有相同的哈希值 67 关系代数运算 ¨ 差运算 ¤ 同一个模式的关系R和关系T求差（R-T） ¤ Map阶段：R中的记录r输出键值对(r, R)，T中的记录r输出键值对(r, T) ¤ Reduce阶段：如果只有R而没有T，则将该记录输出 ¤ 注意事项：R和T的相同记录要发送到同一个Reduce节点 n 重写hashCode()方法使得具有相同域值的记录具有相同的哈希值 68 关系代数运算 ¨ 自然连接 ¤ 比如在属性ID上做关系R和关系S的自然连接 ¤ Map阶段：将ID的值作为key，将其余属性的值以及R的名称（S中的记 录为S的名称）作为value ¤ Reduce阶段：将同一key中所有的值根据它们的来源（R或S）分为两组 做笛卡尔乘积然后输出。 ¨ 代码示例 69","libVersion":"0.3.2","langs":""}