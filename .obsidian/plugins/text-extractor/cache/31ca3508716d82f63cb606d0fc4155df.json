{"path":"docs/学校课程/归档课程/大数据/课件/04 Google MapReduce基本架构.pdf","text":"Google MapReduce基本架构 Google的三驾马车 ¨ SOSP2003 The Google File System ¨ OSDI2004 MapReduce: simplified data processing on large clusters ¨ OSDI2006 Bigtable: a distributed storage system for structured data 2 GFS MapReduce Bigtable 摘要 ¨ Google MapReduce的基本工作原理 ¨ 分布式文件系统GFS的基本工作原理 ¨ 分布式结构化数据表BigTable的基本工作原理 摘要 ¨ Google MapReduce的基本工作原理 ¨ 分布式文件系统GFS的基本工作原理 ¨ 分布式结构化数据表BigTable的基本工作原理 Google MapReduce并行处理的基本过程 5 Cite from Dean and Ghemawat (OSDI 2004) 1.有一个待处理的大 数据，被划分为大 小相同的数据块(如 64MB),及与此相应 的用户作业程序 2.系统中有一个负责调 度的主节点(Master),以 及数据Map和Reduce工 作节点(Worker) 6 Cite from Dean and Ghemawat (OSDI 2004) 3.用户作业程序提交 给主节点 4.主节点为作业程序 寻找和配备可用的 Map节点，并将程 序传送给Map节点 5.主节点也为作业程序 寻找和配备可用的 Reduce节点，并将程 序传送给Reduce节点 Google MapReduce并行处理的基本过程 7 Cite from Dean and Ghemawat (OSDI 2004) 6.主节点启动每个 Map节点执行程序， 每个Map节点尽可 能读取本地或本机 架的数据进行计算 7.每个Map节点处理读取的 数据块,并做一些数据整理 工作(combining, sorting等) 并将中间结果存放在本地； 同时通知主节点计算任务 完成并告知中间结果数据 存储位置 Google MapReduce并行处理的基本过程 8 Cite from Dean and Ghemawat (OSDI 2004) 8.主节点等所有Map节 点计算完成后，开始 启动Reduce节点运行； Reduce节点从主节点 所掌握的中间结果数 据位置信息，远程读 取这些数据 9.Reduce节点计算结果汇 总输出到一个结果文件 即获得整个处理结果 Google MapReduce并行处理的基本过程 9 Cite from Dean and Ghemawat (OSDI 2004) 完整计算过程 Google MapReduce并行处理的基本过程失效处理 ¨ 主节点失效 ¤ 主节点中会周期性地设置检查点(checkpoint)，检查整个计算作业的执行情况，一旦某 个任务失效，可以从最近有效的检查点开始重新执行，避免从头开始计算的时间浪费。 ¤ 如果只有一个Master，它不太可能失败；因此，如果Master失败，将中止MapReduce计 算。 ¨ 工作节点失效 ¤ 工作节点失效是很普遍发生的，主节点会周期性地给工作节点发送检测命令，如果工 作节点没有回应，这认为该工作节点失效，主节点将终止该工作节点的任务并把失效 的任务重新调度到其它工作节点上重新执行。 10 带宽优化 ¨ 问题 ¤ 大量的键值对数据在传送给Reduce节点时会引起较大的通信带宽开销。 ¨ 解决方案 ¤ 每个Map节点处理完成的中间键值对将由combiner做一个合并压缩，即把那 些键名相同的键值对归并为一个键名下的一组数值。 11 (good, 1) (weather, 1) (is, 1) (good, 1) (good, 2) (weather, 1) (is, 1) combiner 计算优化 ¨问题 ¤Reduce节点必须要等到所有Map节点计算结束才能开始执行，因此，如果有一个计算量大、 或者由于某个问题导致很慢结束的Map节点，则会成为严重的“拖后腿者”。 ¨解决方案 ¤把一个Map计算任务让多个Map节点同时做，取最快完成者的计算结果。 12 根据Google的测试，使用了这个冗余Map节 点计算方法以后，计算任务性能提高40%多！ 用数据分区解决数据相关性问题 ¨ 问题 ¤ 一个Reduce节点上的计算数据可能会来自多个Map节点，因此，为了在进入 Reduce节点计算之前，需要把属于一个Reduce节点的数据归并到一起。 ¨ 解决方案 ¤ 在Map阶段进行了Combining以后，可以根据一定的策略对Map输出的中间结 果进行分区(partitioning)，这样即可解决以上数据相关性问题避免Reduce计 算过程中的数据通信。 n 例如：有一个巨大的数组，其最终结果需要排序，每个Map节点数据处理好后，为了避免在每个 Reduce节点本地排序完成后还需要进行全局排序，我们可以使用一个分区策略如:(d%R)，d为数据 大小，R为Reduce节点的个数，则可根据数据的大小将其划分到指定数据范围的Reduce节点上，每 个Reduce将本地数据排好序后即为最终结果。 13 摘要 ¨ Google MapReduce的基本工作原理 ¨ 分布式文件系统GFS的基本工作原理 ¨ 分布式结构化数据表BigTable的基本工作原理 分布式文件系统GFS的工作原理 ¨ 海量数据怎么存储？数据存储可靠性怎么解决？ ¨ 主流的分布文件系统有： ¤ RedHat的GFS ¤ IBM的GPFS ¤ Sun的Lustre等 ¨ 主要用于对硬件设施要求很高的高性能计算或大型数据中心； ¨ 价格昂贵且缺少完整的数据存储容错解决方案 ¨ 如Lustre只对元数据管理提供容错处理，但对于具体的分布存储节点，可靠性完全依赖于这些 分布节点采用RAID或存储区域网(SAN)技术提供容错，一旦分布节点失效，数据就无法恢复。 15 Google GFS的基本设计原则 ¨ Google GFS是一个基于分布式集群的大型分布式文件系统，为 MapReduce计算框架提供数据存储和数据可靠性支撑； ¨ GFS是一个构建在分布节点本地文件系统之上的一个逻辑上文件系 统，它将数据存储在物理上分布的每个节点上，但通过GFS将整个 数据形成一个逻辑上整体的文件。 16 Google GFS的基本设计原则 17 …… Google GFS Google MapReduce MapReduce Applications Google GFS的基本设计原则 ¨ 廉价本地磁盘分布存储 ¤ 各节点本地分布式存储数据，优点是不需要采用价格较贵的集中式磁盘阵列，容量可 随节点数增加自动增加 ¨ 多数据自动备份解决可靠性 ¤ 采用廉价的普通磁盘，把磁盘数据出错视为常态，用自动多数据备份存储解决数据存 储可靠性问题 ¨ 为上层的MapReduce计算框架提供支撑 ¤ GFS作为向上层MapReduce执行框架的底层数据存储支撑，负责处理所有的数据自动存 储和容错处理，因而上层框架不需要考虑底层的数据存储和数据容错问题 18 Google GFS的基本架构和工作原理 19 Cite from Ghemawat et al. (SOSP 2003) GFS Master ChunkServer GFS Master Master上保存了GFS文件系统的 三种元数据 ： ¨ 命名空间(Name Space)即整个 分布式文件系统的目录结构 ¨ Chunk与文件名的映射表 ¨ Chunk副本的位置信息，每一个Chunk默认有3个副本 20 GFS Master • 前两种元数据可通过操作日志提供容错处理能力； • 第3个元数据直接保存在ChunkServer上，Master启动或Chunk Server注册 时自动完成在Chunk Server上元数据的生成； • 因此，当Master失效时，只要ChunkServer数据保存完好，可迅速恢复 Master上的元数据。 Google GFS的基本架构和工作原理 ¨ GFS中每个数据块划分缺省为64MB ¨ 每个数据块会分别在3个(缺省情况下)不同的地 方复制副本； ¨ 对每一个数据块，仅当3个副本都更新成功时， 才认为数据保存成功。 21 ¨ 当某个副本失效时，Master会自动将正确的副本数据进行复制以保证足够的 副本数； ¨ GFS上存储的数据块副本，在物理上以一个本地的Linux操作系统的文件形式 存储，每一个数据块再划分为64KB的子块，每个子块有一个32位的校验和， 读数据时会检查校验和以保证使用未失效的数据。 ChunkServer GFS ChunkServer：即用来保存大量实际数据的数据服务器。 Google GFS的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 1.在程序运行前，数据已经存储在GFS文件系统中；程序运行时应用程序会告诉GFS Server所要访问的文件名或者数据块索引是什么 22 Google GFS的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 2. GFS Server根据文件名和数据块索引在其文件目录空间中查找和定位该文件或数据块， 找出数据块具体在哪些ChunkServer上；将这些位置信息回送给应用程序 23 Google GFS的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 3.应用程序根据GFS Server返回的具体Chunk数据块位置信息，直接访问相应的 ChunkServer 24 Google GFS的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 4.应用程序根据GFS Server返回的具体Chunk数据块位置信息直接读取指定位置的数据 进行计算处理 25 Google GFS的基本架构和工作原理 ¨ 数据访问工作过程 ¤特点：应用程序访问具体数据时不需要经过GFS Master，因此，避免了Master成为访问瓶颈 ¤并发访问：由于一个大数据会存储在不同的ChunkServer中，应用程序可实现并发访问 26 Google GFS的基本架构和工作原理 ¨ GFS的系统管理技术 ¤ 大规模集群安装技术：如何在一个成千上万个节点的集群上迅速部署GFS， 升级管理和维护等 ¤ 故障检测技术：GFS是构建在不可靠的廉价计算机之上的文件系统，节点数 多，故障频繁，如何快速检测、定位、恢复或隔离故障节点 ¤ 节点动态加入技术：当新的节点加入时，需要能自动安装和部署GFS ¤ 节能技术：服务器的耗电成本大于购买成本，Google为每个节点服务器配 置了蓄电池替代UPS，大大节省了能耗。 27 Google GFS的基本架构和工作原理摘要 ¨ Google MapReduce的基本工作原理 ¨ 分布式文件系统GFS的基本工作原理 ¨ 分布式结构化数据表BigTable的基本工作原理 BigTable的基本作用和设计思想 ¨ GFS是一个文件系统，难以提供对结构化数据的存储和访问管理。为此， Google在GFS之上又设计了一个结构化数据存储和访问管理系统—BigTable， 为应用程序提供比单纯的文件系统更方便、更高层的数据操作能力。 ¨ Google的很多数据，包括Web索引、卫星图像数据、地图数据等都以结构化形 式存放在BigTable中。 ¨ BigTable提供了一定粒度的结构化数据操作能力，主要解决一些大型媒体数据 （Web文档、图片等）的结构化存储问题。但与传统的关系数据库相比，其结 构化粒度没有那么高，也没有事务处理等能力，因此，它并不是真正意义上的 数据库。 29 BigTable的设计动机和目标  需要存储多种数据 n Google提供的服务很多，需要处理的数据类型也很多，如URL，网页，图片，地 图数据，email，用户的个性化设置等  海量的服务请求 n Google是目前世界上最繁忙的系统，因此，需要有高性能的请求和数据处理能 力  商用数据库无法适用 n 在如此庞大的分布集群上难以有效部署商用数据库系统，且其难以承受如此巨 量的数据存储和操作需求 30 BigTable的设计动机和目标  广泛的适用性：为一系列服务和应用而设计的数据存储系统，可满足对 不同类型数据的存储和操作需求  很强的可扩展性：根据需要可随时自动加入或撤销服务器节点  高吞吐量数据访问：提供P级数据存储能力，每秒数百万次的访问请求  高可用性和容错性：保证系统在各种情况下都能正常运转，服务不中断  自动管理能力：自动加入和撤销服务器，自动负载平衡  简单性：系统设计尽量简单以减少复杂性和出错率 31 BigTable数据模型 ¨ BigTable主要是一个分布式多维表，表中的数据通过： ¤ 一个行关键字（row key） ¤ 一个列关键字（column key） ¤ 一个时间戳（timestamp） 进行索引和查询定位的。 p BigTable对存储在表中的数据不做任何解释，一律视为字节串， 具体数据结构的实现由用户自行定义。 p BigTable查询模型 p (row:string, column:string,time:int64)à 结果数据字节串 p 支持查询、插入和删除操作 32 BigTable数据模型– WebTable Example 33 A large collection of web pages and related information A Table in BigTable is a: Sparse, Distributed, Persistent, Multidimensional Sorted map. BigTable数据存储格式 34 Row Key p 行(Row):大小不超过64KB的任意字符串。表中的数据都是根据行关键字进 行排序的。 p com.cnn.www就是一个行关键字，指明一行存储数据。URL地址倒排好处是：1)同一地 址的网页将被存储在表中连续的位置，便于查找；2)倒排便于数据压缩，可大幅提高数 据压缩率 p 子表(Tablet)：一个大表可能太大，不利于存储管理，将在水平方向上被分为 多个子表 BigTable数据存储格式 35 Column FamilyColumn family is the unit of access control BigTable数据存储格式 36 Column Column key is specified by “Column family : qualifier” BigTable数据存储格式 37 Column p 列(Column): BigTable将列关键字组织成为“列族”(column family)，每个 族中的数据属于同一类别，如anchor是一个列族，其下可有不同的表示一 个个超链的列关键字。一个列族下的数据会被压缩在一起存放（按列存 放）。因此，一个列关键字可表示为： 族名：列名(family:qualifier) p content、anchor都是族名；而cnnsi.com和my.look.ca则是anchor族中 的列名。 BigTable数据存储格式 38 timestamp p 时间戳(time stamp):很多时候同一个URL的网页会不断更新，而Google需 要保存不同时间的网页数据，因此需要使用时间戳来加以区分。 p 为了简化不同版本的数据管理，BigTable提供给了两种设置： p 保留最近的n个版本数据 p 保留限定时间内的所有不同版本数据 BigTable数据存储格式 39 Cell Cell: the storage referenced by a particular row key, column key, and timestamp BigTable基本架构 40 BigTable 主服务器 BigTable客户端 BigTable客户端 程序库 BigTable 子表服务器 BigTable 子表服务器 BigTable 子表服务器 BigTable 子表服务器… … 执行元数据操 作和负载平衡 数据存储和 访问操作 数据存储和 访问操作 数据存储和 访问操作 数据存储和 访问操作 GFS Chubby服务器 （分布式锁服务） GoogleWorkQueue 负责故障监控和处理 子表数据的存储及日志 元数据存储及主服务器选择 BigTable基本架构 41 主服 务器 新子表 分配 子表服务 器间的负 载均衡 子表服务 器状态 监控 ¤ 主服务器 l 新子表分配：当一个新子表产生时，主服务器通过 加载命令将其分配给一个空间足够大的子表服务器； 创建新表、表合并及较大子表的分裂都会产生新的 子表。 l 子表监控：通过Chubby完成。所有子表服务器基本 信息被保存在Chubby中的服务器目录中主服务器检 测这个目录可获取最新子表服务器的状态信息。当 子表服务器出现故障，主服务器将终止该子表服务 器，并将其上的全部子表数据移动到其它子表服务 器。 l 负载均衡：当主服务器发现某个子表服务器负载过 重时，将自动对其进行负载均衡操作。 BigTable基本架构 42 ¤ 子表服务器 n BigTable中的数据都以子表形式保存在子 表服务器上，客户端程序也直接和子表服 务器通信。 n 分配：当一个新子表产生，子表服务器的 主要问题包括子表的定位、分配、及子表 数据的最终存储。 ¤ 子表的基本存储结构SSTable n SSTable是BigTable内部的基本存储结构，以GFS文件形式存储在GFS文件系 统中。一个SSTable实际上对应于GFS中的一个64MB的数据块(Chunk) n SSTable中的数据进一步划分为64KB的子块，因此一个SSTable可以有多达1 千个这样的子块。为了维护这些子块的位置信息，需要使用一个Index索引。 Index 64K block 64K block 64K block SSTable BigTable基本架构 43 ¤ 子表数据格式 n 概念上子表是整个大表的多行数据划分后构成。而一个子表服务器上 的子表将进一步由很多个SSTable构成，每个SSTable构成最终的在底层 GFS中的存储单位。 Index 64K block 64K block 64K block SSTable Index 64K block 64K block 64K block SSTable Tablet Start:aardvark End:apple BigTable基本架构 44 ¤ 子表数据格式 n 一个SSTable还可以为不同的子表所共享，以避免同样数据的重复存储。 SSTable SSTable SSTable SSTable Tablet aardvark apple Tablet apple_two_E boat BigTable基本架构 45 ¤ 子表寻址 n 子表地址以3级B+树形式进行索引；首先从Chubby服务器中取得根子表， 由根子表找到二级索引子表，最后获取最终的SSTable的位置 参考文献 1. Jeffrey Dean and Sanjay Ghemawat, MapReduce: Simplied Data Processing on Large Clusters, Proceedings of OSDI 2004. 2. Sanjay Ghemawat, et. al, The Google File System, Proceedings of the 19th ACM Symposium on Operating systems principles, SOSP’03, Oct. 2003. 3. Fay Chang, et. al, Bigtable: a distributed storage system for structured data, Proceedings of OSDI 2006. 46 THANK YOU","libVersion":"0.3.2","langs":""}