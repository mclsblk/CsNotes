{"path":"docs/学校课程/归档课程/大数据/课件/08 MapReduce Advanced Programming.pdf","text":"MapReduce高级程序设计 摘要 p 复合键值对的使用 p 用户自定义数据类型 p 用户自定义输入输出格式 p 用户自定义Partitioner和Combiner p 迭代完成MapReduce计算 p 链式MapReduce任务 p 全局参数/数据文件的传递 p 其它处理技术 2 复合键值对的使用 ¨ 思路：用复合键让系统完成排序 ¤ 问题：map计算过程结束后进行Partitioning处理时，系统自动按照map的输 出键进行排序，因此，进入Reduce节点的(key, [value])对将保证是按照key进 行排序的，而[value]则不保证是排好序的。为了解决这个问题，可以在 Reduce过程中对[value]列表中的各个value进行本地排序。但当[value]列表数 据量巨大、无法在本地内存中进行排序时，将出现问题。 ¤ 改进方法：将value中需要排序的部分加入到key中形成复合键，这样将能利 用MapReduce系统的排序功能完成排序。 ¤ 代价：但需要实现一个新的Partitioner，保证原来同一key值的键值对最后分 区到同一个Reduce节点上。 3 复合键值对的使用 ¨ 带频率的倒排索引示例 1: class Mapper 2: procedure Map(docid n, doc d) 3: H ← new AssociativeArray 4: for all term t ∈ doc d do 5: H{t} ← H{t} + 1 6: for all term t ∈ H do 7: Emit(term t, posting <n, H{t}>) 1: class Reducer 2: procedure Reduce(term t, postings [<n1, f1>, <n2, f2>…]) 3: P ← new List 4: for all posting <a, f> ∈ postings [<n1, f1>, <n2, f2>…] do 5: Append(P, <a, f>) 6: Sort(P)；// 进入Reduce节点的postings不保证按照文档序 号排序,因而需要对postings进行一个本地排序 7: Emit(term t; postings P) 4 复合键值对的使用 ¨ 带频率的倒排索引示例 ¤ 为了能利用系统自动对docid进行排序，解决方法是：代之以生成（term， <docid, tf>）键值对，map时将term和docid组合起来形成复合键<term, docid>。 ¤ 但会引起新的问题，同一个term下的所有posting信息无法被分区 到同一个Reduce节点，为此，需要实现一个新的Partitioner：从 <term, docid>中取出term，以term作为key进行分区。 5 Customized Partitioner 进入reduce的键值对按照(term, docid)排序 复合键值对的使用 ¨ 思路：把小的键值对合并成大的键值对 ¤ 通常一个计算问题会产生大量的键值对，为了减少键值对传输和 排序的开销，一些问题中的大量小的键值对可以被合并成一些大 的键值对(pairs->stripes)。 8 复合键值对的使用 ¨ 例如：单词同现矩阵算法 ¤ 一个Map可能会产生单词a与其它单词间的多个键值对，这些键值对可以在 Map过程中合并成右侧的一个大的键值对(条): ¤ 然后，在Reduce阶段，把每个单词a的键值对(条)进行累加： 9 (a, b) → 1 (a, c) → 2 (a, d) → 5 (a, e) → 3 (a, f) → 2 a → { b: 1, c: 2, d: 5, e: 3, f: 2 } a → { b: 1, d: 5, e: 3 } a → { b: 1, c: 2, d: 2, f: 2 } a → { b: 2, c: 2, d: 7, e: 3, f: 2 } + 复合键值对的使用 ¨ 单词同现矩阵算法 10 Cluster size: 38 cores Data Source: Associated Press Worldstream (APW) of the English Gigaword Corpus (v3), which contains 2.27 million documents (1.8 GB compressed, 5.7 GB uncompressed) 用户自定义数据类型 ¨ Hadoop内置的数据类型，这些数据类型都实现了WritableComparable接口，以 便进行网络传输和文件存储，以及进行大小比较。 11 Class Description BooleanWritable Wrapper for a standard Boolean variable ByteWritable Wrapper for a single byte DoubleWritable Wrapper for a Double FloatWritable Wrapper for a Float IntWritable Wrapper for a Integer LongWritable Wrapper for a Long NullWritable Placeholder when the key or value is not needed Text Wrapper to store text using the UTF-8 format 用户自定义数据类型 ¨ 需要实现Writable接口，作为key或者需要比较大小时则需要实现 WritableComparable接口。 12 public class Point3D implements WritableComparable <Point3D>{ private int x, y, z; public int getX() { return x; } public int getY() { return y; } public int getZ() { return z; } public void write(DataOutput out) throws IOException{ out.writeFloat(x); out.writeFloat(y); out.writeFloat(z); } public void readFields(DataInput in) throws IOException{ x = in.readFloat(); y = in.readFloat(); z = in.readFloat(); } public int compareTo(Point3D p){ //compares this(x, y, z) with p(x, y, z) and //outputs -1(小于), 0(等于), 1(大于) } } 用户自定义数据类型 public class Edge implements WritableComparable<Edge> { private String departureNode; private String arrivalNode; public String getDepartureNode() { return departureNode;} @Override public void readFields(DataInput in) throws IOException { departureNode = in.readUTF(); arrivalNode = in.readUTF(); } @Override public void write(DataOutput out) throws IOException { out.writeUTF(departureNode); out.writeUTF(arrivalNode); } @Override public int compareTo(Edge o) { return (departureNode.compareTo(o.departureNode)!=0) ?departureNode.compareTo(o.departureNode):arrivalNode.compareTo(o.arrivalNode); } } 13 用户自定义输入输出格式 ¨ 数据输入格式（InputFormat）用于描述MapReduce作业的数据输 入规范。 ¨ MapReduce框架依靠数据输入格式完成输入规范检查（比如输入 文件目录的检查）、对数据文件进行输入分片（InputSplit），以 及提供从输入分块中将数据记录逐一读出，并转换为Map过程的 输入键值对等功能。 ¨ TextInputFormat是系统缺省的数据输入格式。 14 用户自定义输入输出格式 ¨ Hadoop内置的文件输入格式 15 InputFormat: Description: Key: Value: TextInputFormat Default format; reads lines of text files The byte offset of the line The line contents KeyValueTextInput Format Parses lines into key- val pairs Everything up to the first tab character The remainder of the line SequenceFileInput Format A Hadoop-specific high-performance binary format user-defined user-defined 用户自定义输入输出格式 ¨ Hadoop内置的文件输入格式 ¨ AutoInputFormat, CombineFileInutFormat, CompositeInputFormat, DBInputFormat, FileInputFormat, KeyValueTextInputFormat, LineDocInputFormat, MultiFileInputFormat, NLineInputFormat, SequenceFileAsBinaryInputFormat, SequenceFileAsTextInputFormat, SequenceFileInputFilter, SequenceFileInputFormat, StreamInputFormat, TextInputFormat 16 用户自定义输入输出格式 ¨ Hadoop内置的RecordReader 17 RecordReader: InputFormat Description: LineRecordReader default reader for TextInputFormat reads lines of text files KeyValueLineRecordReader default reader for KeyValueTextInputFormat parses lines into key-val pairs SequenceFileRecordReader default reader for SequenceFileInput Format User-defined methods to create keys and values 用户自定义输入输出格式 ¨ Hadoop内置的RecordReader ¨ CombineFileRecordReader, DBInputFormat.DBRecordReader, InnerJoinRecordReader, JoinRecordReader, KeyValueLineRecordReader, LineDocRecordReader, MultiFilterRecordReader, OuterJoinRecordReader, OverrideRecordReader, SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader, SequenceFileAsTextRecordReader, SequenceFileRecordReader, StreamBaseRecordReader, StreamXmlRecordReader, WrappedRecordReader 18 用户自定义输入输出格式 ¨ 用户自定义InputFormat和RecordReader 19 import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class InvertedIndexMapper extends Mapper<Text, Text, Text, Text> { @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException // default RecordReader: LineRecordReader; // key: line offset; value: line string { Text word = new Text(); FileSplit fileSplit = (FileSplit)context.getInputSplit(); String fileName = fileSplit.getPath().getName(); Text fileName_lineOffset = new Text(fileName+”@”+key.toString()); StringTokenizer itr = new StringTokenizer(value.toString()); for(; itr.hasMoreTokens(); ) { word.set(itr.nextToken()); context.write(word, fileName_lineOffset); } } } 简单的文档倒排索引 由于采用了缺省的 TextInputFormat和 LineRecordReader， 需要增加此段代码 完成特殊处理 用户自定义输入输出格式 ¨ 用户自定义InputFormat和RecordReader 20 public class FileNameLocInputFormat extends FileInputFormat<Text, Text> { @Override public RecordReader<Text, Text> createRecordReader(InputSplit split, TaskAttemptContext context) { FileNameLocRecordReader fnrr = new FileNameRecordReader(); try { fnrr.initialize(split, context); } catch (IOException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } return fnrr; } } 简单的文档倒排索引 可以自定义一个InputFormat和RecordReader实现同样的效果 用户自定义输入输出格式 21 public class FileNameLocRecordReader extends RecordReader<Text, Text> { String fileName; LineRecordReader lrr = new LineRecordReader(); …… @override public Text getCurrentKey() throws IOException, InterruptedException { return new Text(\"(\" + fileName + “@\" + lrr.getCurrentKey() + \")\"); } @override public Text getCurrentValue() throws IOException, InterruptedException { return lrr.getCurrentValue(); } @override public void initialize(InputSplit arg0, TaskAttemptContext arg1) throws IOException, InterruptedException { lrr.initialize(arg0, arg1); fileName = ((FileSplit)arg0).getPath().getName(); } } 用户自定义输入输出格式 22 public class InvertedIndexer { public static void main(String[] args) { try { Configuration conf = new Configuration(); job = new Job(conf, \"invert index\"); job.setJarByClass(InvertedIndexer.class); job.setInputFormatClass(FileNameLocInputFormat.class); job.setMapperClass(InvertedIndexMapper.class); job.setReducerClass(InvertedIndexReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } catch (Exception e) { e.printStackTrace(); } } } 用户自定义输入输出格式 23 import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class InvertedIndexMapper extends Mapper<Text, Text, Text, Text> { @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException // InputFormat: FileNameLocInputFormat // RecordReader: FileNameLocRecordReader // key: filename@lineoffset; value: line string { Text word = new Text(); StringTokenizer itr = new StringTokenizer(value.toString()); for(; itr.hasMoreTokens(); ) { word.set(itr.nextToken()); context.write(word, key); } } } 用户自定义输入输出格式 ¨ 数据输出格式（OutputFormat）用于描述MapReduce作业的数据 输出规范。 ¨ MapReduce框架依据数据输出格式完成输出规范检查（如检查输 出目录是否存在）以及提供作业结果数据输出等功能。 ¨ TextOutputFormat是系统缺省的数据输出格式。 24 用户自定义输入输出格式 ¨ Hadoop内置的OutputFormat和RecordWriter 25 OutputFormat: Description TextOutputFormat Default; writes lines in \"key \\t value\" form SequenceFileOutputFormat Writes binary files suitable for reading into subsequent MapReduce jobs NullOutputFormat Disregards its outputs DBOutputFormat, FileOutputFormat, FilterOutputFormat, IndexUpdateOutputFormat, LazyOutputFormat, MapFileOutputFormat, MultipleOutputFormat, MultipleSequenceFileOutputFormat, MultipleTextOutputFormat, NullOutputFormat, SequenceFileAsBinaryOutputFormat, SequenceFileOutputFormat, TextOutputFormat 用户自定义输入输出格式 ¨ Hadoop内置的OutputFormat和RecordWriter 26 RecordWriter: Description LineRecordWriter Default RecordWriter for TextOutputFormat writes lines in \"key \\t value\" form DBOutputFormat.DBRecordWriter, FilterOutputFormat.FilterRecordWriter, TextOutputFormat.LineRecordWriter 与InputFormat和RecordReader类似，用户可以根据需要定制OutputFormat和 RecordWriter 用户自定义输入输出格式 ¨ 划分多个输出文件集合 ¤ 缺省情况下，MapReduce将产生包含一至多个文件的单个输出数据文件集合。但有时候作 业可能需要输出多个文件结合。 n 比如：在处理巨大的访问日志文件时，由于文件太大我们可能希望按每天的日期将访问 日志记录输出为每天日期下的文件。在处理专利数据集时，我们希望根据不同国家，将 每个国家的专利数据记录输出到不同国家的文件目录中。 n Hadoop提供了MultipleOutputFormat类(org.apache.hadoop.mapred.lib.MultipleOutputFormat) 来快速完成这一处理功能。在Reduce进行数据输出前， MultipleOutputFormat将调用一个 内部方法以决定输出的文件名是什么。通常需要继承并实现MultipleOutputFormat的一个 子类并实现其中的generateFileNameForKeyValue()方法以根据当前的键值对由程序产生并 返回一个输出文件路径： protected String generateFileNameForKeyValue(K key, V value, String name) 27 用户自定义输入输出格式 ¨ 划分多个输出文件集合 ¤ 例如：将专利描述文件数据集按照国家进行多文件集合输出 “PATENT”,”GYEAR”,”GDATE”,”APPYEAR”,”COUNTRY”, ”POSTATE”,”ASSIGNEE”, ”ASSCODE”,”CLAIMS”,”NCLAS S”,”CAT”,”SUBCAT”,”CMADE”,”CRECEIVE”, ”RATIOCIT”,”GENERAL”,”ORIGINAL”,”FWDAPLAG”,”BCKGTLAG”,”SELFCT UB”, ”SELFCTLB”,”SECDUPBD”,”SECDLWBD” 3070801,1963,1096,,”BE”,””,,1,,269,6,69,,1,,0,,,,,,, 3070802,1963,1096,,”US”,”TX”,,1,,2,6,63,,0,,,,,,,,, 28 用户自定义输入输出格式 ¨ 划分多个输出文件集合 ¤ 例如：将专利描述文件数据集按照国家进行多文件集合输出 public static class MapClass extends Mapper<LongWritable, Text, NullWritable, Text> { public void map(LongWritable key, Text value, Context context） throws IOException, InterruptedException { context.write (NullWritable.get(), value); } // NullWritable.get() 返回singleton单一实例 } public static class SaveByCountryOutputFormat extends MultipleTextOutputFormat<NullWritable,Text> { protected String generateFileNameForKeyValue (NullWritable key, Text value, String filename) { String[] arr = value.toString().split(“,”, -1); String country = arr[4].substring(1,3); return country + “/” + filename; } } 29 用户自定义输入输出格式 ¨ 划分多个输出文件集合 public class MultiFileDemo { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = new Job(conf, MultiFileDemo.class); Path in = new Path(args[0]); Path out = new Path(args[1]); FileInputFormat.setInputPaths(job, in); FileOutputFormat.setOutputPath(job, out); job.setJobName(“MultiFileDemo”); job.setMapperClass(MapClass.class); job.setInputFormat(TextInputFormat.class); job.setOutputFormat(SaveByCountryOutputFormat.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class); job.setNumReduceTasks(0); Job.waitForCompletion(true); } } 30 用户自定义输入输出格式 ¨ 执行结果 31 用户自定义输入输出格式 ¨ MultipleOutputFormat (org.apache.hadoop.mapred.lib.MultipleOutputFormat) ¤ MultipleOutputFormat是Hadoop的OutputFormat的一个扩展，用于处理多个输出文件。 ¤ 它允许你为每个Reducer任务定义一个不同的OutputFormat。这意味着你可以为每个Reducer 任务指定不同的输出目录和输出文件格式。 ¤ 这对于根据特定的数据或逻辑将数据分发到不同的输出目录非常有用，更适合在整个 MapReduce作业级别控制多个输出文件的格式和目录。 ¨ MultipleOutputs (org.apache.hadoop.mapreduce.lib.output.MultipleOutputs) ¤ MultipleOutputs是一个更高级别的API，用于在Mapper或Reducer内部根据某些条件将数据输 出到多个文件或目录。 ¤ 它允许你在Mapper或Reducer内部为不同的输出文件指定不同的键值对，而不需要为每个 Reducer任务创建不同的OutputFormat。 ¤ 这对于根据数据的属性或业务逻辑将数据分发到多个输出目录非常有用，而无需创建多个 Reducer任务，更适合在Mapper或Reducer内部根据条件动态控制多个输出文件。 32 用户自定义输入输出格式 ¨ org.apache.hadoop.mapred.* vs org.apache.hadoop.mapreduce.* ¨ 历史演变: ¤ mapred包是Hadoop的早期版本中使用的包，用于实现MapReduce编程模型。 ¤ mapreduce包是Hadoop 0.20版本之后引入的，用于替代mapred包。这一改变是为了改 进和优化MapReduce的性能以及提供更灵活的编程接口。 ¨ 性能优化: ¤ mapreduce包相对于mapred包进行了许多性能优化，包括更好的资源管理、任务调度、 错误处理等方面的改进，以提高MapReduce作业的执行效率。 ¨ API灵活性: ¤ mapreduce包引入了一种新的API，使得编写MapReduce作业更加灵活和容易。这一API 的设计更加现代化，与标准Java编程实践更加一致，因此编写和维护MapReduce作业变 得更容易。 33 用户自定义Partitioner和Combiner ¨ 定制Partitioner ¤ 程序员可以根据需要定制Partitioner来改变Map中间结果到Reduce节点的分区方式，并在Job 中设置新的Partitioner 34 class NewPartitioner extends HashPartitioner<K,V> { // override the method getPartition(K key, V value, int numReduceTasks) { term = key. toString().split(“,”)[0]; //<term, docid>=>term super.getPartition(term, value, numReduceTasks); } } 并在Job中设置新的Partitioner： Job. setPartitionerClass(NewPartitioner) 用户自定义Partitioner和Combiner ¨ 定制Combiner ¤ 程序员可以根据需要定制Combiner来减少网络数据传输量，提高系统效率， 并在Job中设置新的Combiner 例如，每年申请美国专利的国家数统计 Patent description data set “apat63_99.txt” “PATENT”,”GYEAR”,”GDATE”,”APPYEAR”,”COUNTRY”, ”POSTATE”,”ASSIGNEE”, ”ASSCODE”,”CLAIMS”,”NCLASS”,”CAT”,”SUBCAT”, ”CMADE”,”CRECEIVE”, ”RATIOCIT”,”GENERAL”,”ORIGINAL”,”FWDAPLAG”,”BCKGTLAG”,”SELFCTUB”, ”SELFCTLB”,”SECDUPBD”, ”SECDLWBD” 3070801,1963,1096,,”BE”,””,,1,,269,6,69,,1,,0,,,,,,, 3070802,1963,1096,,”US”,”TX”,,1,,2,6,63,,0,,,,,,,,, 3070803,1963,1096,,”US”,”IL”,,1,,2,6,63,,9,,0.3704,,,,,,, 3070804,1963,1096,,”US”,”OH”,,1,,2,6,63,,3,,0.6667,,,,,,, 3070805,1963,1096,,”US”,”CA”,,1,,2,6,63,,1,,0,,,,,,, 35 用户自定义Partitioner和Combiner ¨ 定制Combiner 每年申请美国专利的国家数统计 1. Map中用<year, country>作为key输出，Emit(<year, country>,1) (<1963, BE>, 1), (<1963, US>, 1), (<1963, US>, 1), … 2. 实现一个定制的Partitioner，保证同一年份的数据划分到同一个Reduce节点 3. Reduce中对每一个(<year, country>, [1, 1,1,…])输入，忽略后部的出现次数，仅考虑key部分：<year, country> 问题：如每碰到一个<year, country >，即emit(year, 1)有问题吗？ 答案：有问题。因为可能会有从不同Map节点发来的同样的<year, country>，因此会出现对同一国家的重复 计数 解决办法：在Reduce中仅计数同一年份下不同的国家个数 问题： Map结果(<year, country>, [1, 1,1,…])数据通信量较大 解决办法：实现一个Combiner将[1, 1,1,…]合并为1 36 用户自定义Partitioner和Combiner ¨ 定制Combiner 每年申请美国专利的国家数统计 public static class NewCombiner extends Reducer < Text, IntWritable, Text, IntWritable > { public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException { // 忽略(<year, country>, [1,1,1,…])后部大量重复的[1,1,1,…]， // 归并为<year, country>的1次出现 context.write(key, new IntWritable(1)); } // 输出key: <year, country>; value: 1 } 37 迭代MapReduce计算 ¨ 基本问题 ¤ 一些求解计算需要用迭代方法求得逼近结果（求解计算必须是收敛性的）。当用 MapReduce进行这样的问题求解时，运行一趟MapReduce过程无法完成整个求解过程，因此， 需要采用迭代方法循环运行该MapReduce过程，直到达到一个逼近结果。 ¨ 例如：页面排序算法PageRank ¤ 随机浏览模型：假设一位上网者随机地浏览一些网页 • 有可能从当前网页点击一个链接继续浏览（概率为d）； • 有可能随机跳转到其它N个网页中的任一个（概率为1-d）。 ¤ 每个网页的PageRank值可以看成该网页被随机浏览的概率： 38 L(pj)为网页pj上的超链个数 迭代MapReduce计算 ¨ 页面排序算法PageRank ¨ 问题是在求解PR(pi)时，需要递归调用PR(pj)，而PR(pj)本身也是待求解的。因此， 我们只能先给每个网页赋一个假定的PR值，如0.5。但这样求出的PR(pi)肯定不 准确。然而，当用求出的PR值反复进行迭代计算时，会越来越趋近于最终的准 确结果。 ¨ 因此，需要用迭代方法循环运行MapReduce过程，直至第n次迭代后的结果与 第n-1次的结果小于某个指定的阈值时结束，或者通过经验控制循环固定的次 数。 39 多趟MapReduce的处理 40 public class PageRankDriver { private static int times = 10; public static void main(String args[]) throws Exception{ String[] forGB = {\"\", args[1]+\"/Data0\"}; forGB[0] = args[0]; GraphBuilder.main(forGB); String[] forItr = {\"Data\",\"Data\"}; for (int i=0; i<times; i++) { forItr[0] = args[1]+\"/Data\"+(i); forItr[1] = args[1]+\"/Data\"+(i+1); PageRankIter.main(forItr); } String[] forRV = {args[1]+\"/Data\"+times, args[1]+\"/FinalRank\"}; PageRankViewer.main(forRV); } } 链式MapReduce任务 ¨ 基本问题 ¤ 一些复杂任务难以用一趟MapReduce处理过程来完成，需要将其分为多趟简单些的 MapReduce子任务完成。如： ¤ 专利文献引用直方图统计，需要先进行被引次数统计，然后在被引次数上再进行被引直方 图统计 41 “CITING”,”CITED” 3858241,956203 3858241,1324234 3858241,3398406 3858241,3557384 3858241,3634889 3858242,1515701 3858242,3319261 3858242,3668705 3858242,3707004 ... 1 2 （2,1） （2,3） 10000 1 （1,1） （1,9） 100000 1 （1,1） 1000006 1 （1,1） （3,1） 1000007 1 （1,1） 1000011 1 （1,1） 1000017 1 （1,1） 1000026 1 （1,1） 1000033 2 （2,1） 1000043 1 （1,1） 1000044 2 （2,1） 1000045 1 （1,1） 1000046 3 （3,1） …… …… MapReduce 1 MapReduce 2 链式MapReduce任务 ¨ MapReduce子任务的顺序化执行 ¤ 多个MapReduce子任务可以用手工逐一执行，但更方便的做法是将这些子任务穿起来，前 面MapReduce任务的输出作为后面MapReduce的输入，自动地完成顺序化的执行, 如： mapreduce-1à mapreduce-2 à mapreduce-3 à ... 42 MapReduce作业控制执行代码： Configuration jobconf = new Configuration(); job = new Job(jobconf, \"invert index\"); job.setJarByClass(InvertedIndexer.class); …… FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); 链式MapReduce任务 ¨ MapReduce子任务的顺序化执行 ¤ 同样，链式MapReduce中的每个子任务需要穿件独立的jobconf，并按照前后子任务间的输 入输出关系设置输入输出路径，而任务完成后所有中间过程的输出结果路径都可以删除掉。 43 Configuration jobconf1= new Configuration(); job1 = new Job(jobconf1, “Job1\"); job1.setJarByClass(jobclass1); …… FileInputFormat.addInputPath(job1, inpath1); FileOutputFormat.setOutputPath(job1, outpath1); job1.waitForCompletion(true); Configuration jobconf2= new Configuration(); job2 = new Job(jobconf2, “Job2\"); job2.setJarByClass(jobclass2); …… FileInputFormat.addInputPath(job2, outpath1); FileOutputFormat.setOutputPath(job2, outpath2); job2.waitForCompletion(true); Configuration jobconf3= new Configuration(); job3 = new Job(jobconf3, “Job3\"); job3.setJarByClass(jobclass3); …… FileInputFormat.addInputPath(job3, outpath2); FileOutputFormat.setOutputPath(job3, outpath3); job3.waitForCompletion(true); 链式MapReduce任务 ¨ MapReduce前处理和后处理步骤的链式执行 ¤ 一个MapReduce作业可能会有一些前处理和后处理步骤，比如，文档倒排索 引处理前需要一个去除Stop-word的前处理，倒排索引处理后需要一个变形 词后处理步骤(making, made à make)。将这些前后处理步骤实现为单独的 MapReduce任务可以达到目的，但将增加很多I/O操作，因而效率不高。 ¤ 一个办法是在核心的Map和Reduce过程之外，把这些前后处理步骤实现为一 些辅助的Map和Reduce过程，将这些辅助的Map和Reduce过程与核心的Map 和Reduce过程合并为一个过程链，从而完成整个作业。 44 链式MapReduce任务 ¨ MapReduce前处理和后处理步骤的链式执行 ¤ Hadoop提供了链式Mapper(org.apache.hadoop.mapred.lib.ChainMapper)和链式Reducer (org.apache.hadoop.mapred.lib.ChainReducer)来完成这种处理 ¤ ChainMapper和ChainReducer分别提供了addMapper方法加入一系列Mapper： ChainMapper.addMapper（……）; ChainReducer.addMapper（……） public static void addMapper (JobConf job, //主作业 Class<? extends Mapper> mclass, //待加入的map class Class<?> inputKeyClass, //待加入的map输入键class Class<?> inputValueClass, //待加入的map输入键值class Class<?> outputKeyClass, //待加入的map输出键class Class<?> outputValueClass, //待加入的map输出键值class boolean byValue //指示键/值是否应按值传递给链中的下一个Mapper JobConf mapperConf // 待加入的map的conf ) throws IOException 45 链式MapReduce任务 ¨ MapReduce前处理和后处理步骤的链式执行 ¤ 设有一个完整的MapReduce作业，由Map1 , Map2 , Reduce, Map3, Map4构成。 Configuration conf = new Configuration(); Job job = new Job(conf); job.setJobName(“ChainJob”); job.setInputFormat(TextInputFormat.class); job.setOutputFormat(TextOutputFormat.class); FileInputFormat.setInputPaths(job, in); FileOutputFormat.setOutputPath(job, out); JobConf map1Conf = new JobConf(false); ChainMapper.addMapper(job, Map1.class, LongWritable.class, Text.class, Text.class, Text.class, true, map1Conf); JobConf map2Conf = new JobConf(false); ChainMapper.addMapper(job, Map2.class, Text.class, Text.class, LongWritable.class, Text.class, true, map2Conf); 46 链式MapReduce任务 ¨ MapReduce前处理和后处理步骤的链式执行 JobConf reduceConf = new JobConf(false); ChainReducer.setReducer(job, Reduce.class, LongWritable.class, Text.class, Text.class, Text.class, true, reduceConf); JobConf map3Conf = new JobConf(false); ChainReducer.addMapper(job, Map3.class, Text.class, Text.class, LongWritable.class, Text.class, true, map3Conf); JobConf map4Conf = new JobConf(false); ChainReducer.addMapper(job, Map4.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, map4Conf); JobClient.runJob(job); 47 ChainReducer.setReducer()方法必须在ChainReducer最开始的地方使用，其后方可加入后续的辅助处理 Mapper；另一个需要注意的问题是，这些链式Mapper和Reducer之间传递的键值对数据类型必须保持前 后一致。 全局参数/数据文件的传递 ¨ 全局作业参数的传递 ¤ 为了能让用户灵活设置某些作业参数，避免作业参数在程序中的硬编码，一个 MapReduce计算任务可能需要在执行时从命令行输入这些作业参数，并将这些参数 传递给各个计算节点。 ¤ 比如，对两个关系进行自然连接时程序用硬编码方式指定第一个数据列为join的主 键。但为了要实现一个具有一定通用性的程序，可以任意指定一个列为join主键的 话，就需要在程序运行时在命令行中指定join主键所在的数据列。然后该输入参数 可以作为一个属性保存在Configuration对象中，并允许Map和Reduce节点从 Configuration对象中获取和使用该属性值。 48 全局参数/数据文件的传递 ¨ 全局作业参数的传递 Configuration类专门提供以下用于保存和获取属性的方法： ¤ public void set(String name, String value) //设置字符串属性 ¤ public String get(String name) // 读取字符串属性 ¤ public String get(String name, String defaultValue) // 读取字符串属性 ¤ public void setBoolean(String name, boolean value) //设置布尔属性 ¤ public boolean getBoolean(String name, boolean defaultValue) //读取布尔属性 ¤ public void setInt(String name, int value) //设置整数属性 ¤ public int getInt(String name, int defaultValue) // 读取整数属性 ¤ public void setLong(String name, long value) //设置长整数属性 ¤ public long getLong(String name, long defaultValue) // 读取长整数属性 ¤ public void setFloat(String name, float value) //设置浮点数属性 ¤ public float getFloat(String name, float defaultValue) //读取浮点数属性 ¤ public void setStrings(String name, String... values) //设置一组字符串属性 ¤ public String[] getStrings(String name, String... defaultValue) //读取一组字符串属性 ¨ 需要说明的是，setStrings方法将把一组字符串转换为用“,”隔开的一个长字符串，然后getStrings时自动再根据 “,”split成一组字符串，因此，在该组中的每个字符串都不能包含“,”，否则会出错。 49 全局参数/数据文件的传递 ¨ 全局作业参数的传递 例：专利文献数据集Join时主键所在数据列参数的设置 Configuration conf = new Configuration(); Job job = new Job(conf, \"naturalJoinJob\"); ... // 将第三个输入参数设置为Join Key属性 job.getConfiguration().setInt(\"JoinKeyColIdx\", Integer.parseInt(args[2])); …… job.waitForCompletion(true); 50 全局参数/数据文件的传递 ¨ 全局作业参数的传递 ¤ 在mapper类的初始化方法setup()中从configuration对象中读出属性 public static class MapClass extends Mapper <Text, Text, Text, Text> { int join_key_col_idx; protected void setup(Mapper.Context context) { Configuration jobconf = context.getConfiguration()； join_key_col_idx = jobconf.getInt(“JoinKeyColIdx”, -1); // 无值时置为-1 } protected void map(Text key, Text value, Context context) throws IOException, InterruptedException { //使用join_key_col_idx完成数据处理； …… } } 51 全局参数/数据文件的传递 ¨ 全局作业参数的传递 ¤ 同样需要时在reducer类的初始化方法setup()中从configuration对象中读出属性 public static class ReduceClass extends Reducer <Text, Text, Text, Text> { int join_key_col_idx; protected void setup(Mapper.Context context) { Configuration jobconf = context.getConfiguration()； join_key_col_idx = jobconf.getInt(“JoinKeyColIdx”, -1); // 无值时置为-1 } protected void reduce(Text key, Text value, Context context) throws IOException, InterruptedException { //使用join_key_col_idx完成数据处理； …… } } 52 全局参数/数据文件的传递 ¨ 全局数据文件的传递 ¤ 有时候一个MapReduce作业可能会使用一些较小的并且需要复制到各个节点的数据文 件。为此，可以使用DistributedCache文件传递机制，先将这些文件传送到 DistributedCache中，然后各个节点从DistributedCache中将这些文件并复制到本地的文件 系统中使用。具体使用时，为提供访问速度，可将这些较小的文件数据读入内存。 ¤ Job类中：public void addCacheFile(URI uri)：将一个文件放到distributed cache file中 ¤ Mapper或Reducer的context类中： public Path[] getLocalCacheFiles(): 获取设置在distributed cache files中的文件路径，以便 能将这些文件读入到每个节点内存中 53 全局参数/数据文件的传递 ¨ 全局数据文件的传递 ¤ 在作业Configuration时将文件存入Distributed Cache： …… Configuration conf = getConf(); Job job = Job.getInstance(conf, \"word count 2.0\");; // 将命令行参数中的reamingsArgs列表里指定的文件依次放置到distributed cache file中 List<String> otherArgs = new ArrayList<String>(); for (int i = 0; i < remainingArgs.length; ++i) { if (\"-skip\".equals(remainingArgs[i])) { job.addCacheFile(new Path(remainingArgs[++i]).toUri()); job.getConfiguration().setBoolean(\"wordcount.skip.patterns\", true); } else { otherArgs.add(remainingArgs[i]); } } 54 全局参数/数据文件的传递 ¨ 全局数据文件的传递 public static class MapClass extends Mapper<Text, Text, Text, Text> { public void setup(Mapper.Context context) throws IOException, InterruptedException conf = context.getConfiguration(); caseSensitive = conf.getBoolean(\"wordcount.case.sensitive\", true); if (conf.getBoolean(\"wordcount.skip.patterns\", true)) { URI[] patternsURIs = Job.getInstance(conf).getCacheFiles(); if (patternsURIs != null) { for (URI patternsURI : patternsURIs) { Path patternsPath = new Path(patternsURI.getPath()); String patternsFileName = patternsPath.getName().toString(); parseSkipFile(patternsFileName); } } } } 55 其它处理技术 ¨ 查询任务相关信息 ¤ 可以通过Configuration对象，使用预定义的属性名称查询计算作业相关的信息。 56 其它处理技术 ¨ 输入输出到关系数据库 ¤ MapReduce用于处理存储在HDFS中的大规模数据，但现实环境中有很多应用数据保存在关 系数据库中，因此，Hadoop提供了访问关系数据库的能力以便在需要时能用MapReduce技 术处理关系数据库中的数据。这在基于MapReduce进行联机数据分析处理时尤为有用。 ¤ OLTP (online transaction processing) n 联机事务处理：主要是关系数据库应用系统中前台常规的各种事务处理 ¤ OLAP (online analytical processing) n 联机分析处理：主要是进行基于数据仓库的后台数据分析和挖掘，提供优化的客户服务和运营决 策支持 ¤ OLTP与OLAP一般采用分离的数据库，前者数据库负责大量的常规的事务处理，后者用数据 仓库应对大量的数据分析处理负载。 57 其它处理技术 58 OLTP 数据库 OLAP 数据仓库ETL Extract：从OLTP数据库中抽取事务数据 Transform：转换为数据仓库中的数据格式 Load：装载到数据仓库中 企业数据库应用系统 问题：OLAP端基于关系数据库的数据仓库解决方案，在数据量巨大 的情况下，复杂数据分析和挖掘处理的负载很大，速度性能跟不上 其它处理技术 59 OLTP 数据库 OLAP ETL 企业数据库应用系统 解决方案：提供基于MapReduce大规模数据并行处理的OLAP！ 问题：如何从MapReduce访问关系数据库？ Hadoop MapReduce 数据仓库 其它处理技术 ¨ 输入输出到关系数据库 ¤ 从数据库中输入数据 n Hadoop提供了相应的从关系库查询和读取数据的接口(org.apache.hadoop.mapred.lib.db.*) n DBInputFormat：提供从数据库读取数据的格式 n DBWritable：提供读取数据记录的接口 ¤ 虽然Hadoop允许用以上接口从数据库中直接读取数据记录作为MapReduce的输入，但处理 效率不理想，因此，仅适合读取小量数据记录的计算和应用，不适合OLAP数据仓库大量数 据的读取处理。 ¤ 读取大量数据记录一个更好的解决办法是，用数据库中的Dump工具将大量待分析数据输出 为文本数据文件，并上载到HDFS中进行处理。 60 其它处理技术 ¨ 输入输出到关系数据库 ¤ 向数据库中输出计算结果 n 基于数据仓库的数据分析和挖掘输出结果的数据量一般不会太大，因而可能适合于直接向数据库写入。 Hadoop提供了相应的向关系库直接输出计算结果的编程接口 n DBOutputFormat：提供向数据库输出数据的格式 n DBConfiguration：提供数据库配置和创建连接的接口 ¤ 创建数据库连接 n DBConfiguration 类中提供了一个静态方法创建数据库连接: n public static void configureDB(Job job, String driverClass, String dbUrl, String userName, String passwd) ¤ 指定写入的数据表和字段 n DBOutputFormat中提供了一个静态方法完成这一工作： n public static void setOutput(Job job, String tableName, String... fieldNames) 61 其它处理技术 ¨ 输入输出到关系数据库 ¤ 向数据库中输出计算结果 ¤ Configuration示例 Configuration conf = new Configuration(); Job job = new Job(conf, JobClass.class); job.setOutputFormat(DBOutputFormat.class); DBConfiguration.configureDB(job, “com.mysql.jdbc.Driver”, “jdbc:mysql://db.host.com/mydb”, “username”, “password”) DBOutputFormat.setOutput(job, “Events”, “event_id”, “time”); // 向Events表输出event_id和time字段 62 其它处理技术 ¨ 输入输出到关系数据库 ¤ 向数据库中输出计算结果 n 实现DBWritable n 为了实际完成向数据库中数据写入，程序员要实现DBWritable： public class EventsDBWritable implements Writable, DBWritable { private int id; private long timestamp; public void write(DataOutput out) throws IOException { out.writeInt(id); out.writeLong(timestamp); } public void readFields(DataInput in) throws IOException { id = in.readInt(); timestamp = in.readLong(); } public void write(PreparedStatement statement) throws SQLException { statement.setInt(1, id); statement.setLong(2, timestamp); } public void readFields(ResultSet resultSet) throws SQLException { id = resultSet.getInt(1); timestamp = resultSet.getLong(2); } // 除非使用DBInputFormat直接从数据库输入数据,否则readFields方法不会被调用 } 63 THANK YOU","libVersion":"0.3.2","langs":""}